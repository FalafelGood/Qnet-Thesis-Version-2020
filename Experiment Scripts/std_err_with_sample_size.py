from QNET import *
import pandas as pd
import matplotlib.pyplot as plt


def error_expectation_against_percolation_probability():
    """
    Experiment to get the average error against percolation probability for a given graph
    # TODO: Does not find the sample variance... Is this a problem?
    Returns
    -------

    """
    size = 8
    e = 1
    f = 0.975
    square = square_lattice(m=size, n=size, efficiency=e, fidelity=f)

    # Create main data frame for stats info on error expectation and standard deviation
    main = pd.DataFrame(columns=['p', 'err (e)', 'err_std (e)', 'err (f)', 'err_std (f)'])
    # Assume num_steps = 20
    prob_list = np.linspace(0, 1, 20)
    # Update probability column in main with prob_list
    main.update({"p": prob_list})

    # List of DataFrames generated by monte_method
    data_list = []

    std_err_sample_size = 30
    for i in range(std_err_sample_size):
        print(f"-- SAMPLE {i} --" + "\n\n")
        data = monte_method(square, pair_method=fixed_pair, reduction_method=QNET.purify_reduce,
                            data_method=data_method, num_iters= 500, num_steps=20, percolation_range=None)
        data_list.append(data)

    # For each DataFrame, stick together with groupby?
    # Concatenate list of DataFrames
    df = pd.concat(data_list)
    # Order DataFrames by index
    df = df.sort_index()
    # Group by index, take mean for each group
    # Numeric_only must be false. Don't ask why.
    df_means = df.groupby(df.index).mean(numeric_only=False)

    # Save to csv and plot
    df_means.to_csv(path_or_buf="/home/hudson/Documents/Code/err_vs_p.csv")
    plt.plot(df_means['p'], df_means['e (std)'], label="Expectation of Efficiency Error")
    plt.plot(df_means['p'], df_means['f (std)'], label="Expectation of Fidelity Error")
    plt.ylabel("Expected Error")
    plt.xlabel("Probability of Node Deletion")
    plt.title("Expectation of \"Simple Purify\" Errors Against Percolation Probability")
    plt.legend()
    plt.show()
    print(df_means)


def running_variance(Q, pair_method, reduction_method, sample_max):
    """
    For a given graph with a given probability, get an array containing the running variance of the data.
    Parameters
    ----------
    Q: Qnet graph
    pair_method: function
    reduction_method: function
    sample_max: int

    Returns
    -------
    list
    """

    # Build column labels for the main DataFrame from cost_vector
    column_labels = []
    for key in Q.cost_vector:
        column_labels.append(key)
        column_labels.append(key + " (std)")

    # Build main DataFrame
    main = pd.DataFrame(columns=column_labels, index=range(sample_max))
    # offset index by 1 since index represents number of samples
    main.index += 1

    # Collect data for main DataFrame
    for i in range(1, sample_max+1):
        if i % 10 == 0:
            print(f"Sampling {i} number of graphs")

        graph_list = generate_graphs(Q, pair_method, reduction_method, num_iters=i, percolation_prob=0.3)

        # minor contains raw data for all generated graphs for a given data point
        minor = pd.DataFrame()
        for R, pairs in graph_list:
            data = data_method(R, pairs)
            minor = minor.append(pd.Series(data), ignore_index=True)
        # Compress minor DataFrame into mean and unbiased standard error
        mean = minor.mean()
        std = minor.sem()
        # Add (std) qualifiers to standard error variables
        for name, dummy in mean.items():
            std = std.rename({name: name + " (std)"})

        # Update main DataFrame with mean and std
        for name, val in mean.items():
            main.at[i, name] = val
        for name, val in std.items():
            main.at[i, name] = val

    # Add redundent "sample size column for ease in plotting
    main["sample_size"] = main.index
    return main


def cumulative_running_variance(Q, pair_method, reduction_method, sample_max):
    """
    For a given graph with a given probability, get an array containing the running variance of the data.
    Parameters
    ----------
    Q: Qnet graph
    pair_method: function
    reduction_method: function
    sample_max: int

    Returns
    -------
    list
    """

    # Build column labels for the main DataFrame from cost_vector
    column_labels = []
    for key in Q.cost_vector:
        # column_labels.append(key)
        column_labels.append(key)

    # Build main DataFrame containing cumulative data
    main = pd.DataFrame(columns=column_labels, index=range(sample_max))
    # offset index by 1 since index represents number of samples
    main.index += 2

    # Build minor DataFrame containing individual data
    minor = pd.DataFrame(columns=["e", "f"])

    # Collect data for minor DataFrame
    for i in range(2, sample_max + 1):
        if i % 10 == 0:
            print(f"Sampling Graph number {i}")

        graph_list = generate_graphs(Q, pair_method, reduction_method, num_iters=1, percolation_prob=0.3)

        for R, pairs in graph_list:
            data = data_method(R, pairs)
            minor = minor.append(pd.Series(data), ignore_index=True)

        # Compress minor DataFrame into unbiased standard error
        std = minor.sem()

        # Update main DataFrame with "std"
        for name, val in std.items():
            main.at[i, name] = val

    # Add redundant "sample size column for ease in plotting
    main["sample_size"] = main.index
    return main


def running_variance_test():

    square = square_lattice(m=8,n=8, efficiency=1, fidelity=0.975)
    diagonal = get_diagonal_pair_method(dim=2, size=8, G=square)
    thing = cumulative_running_variance(square, pair_method=diagonal, reduction_method=QNET.purify_reduce,
                                        sample_max=1000)

    # Plot
    plt.plot(list(thing["sample_size"]), list(thing["e"]), label="Efficiency")
    plt.plot(list(thing["sample_size"]), list(thing["f"]), label="Fidelity")
    plt.ylabel("Standard Error")
    plt.xlabel("Number of 8x8 Square Lattice Graphs Sampled (e:1, f:0.975)")
    plt.title("Error Rates of Simple Purify Method Against Sample Size")
    plt.legend()
    plt.show()


def running_variance_for_different_graph_sizes():
    for size in range(50, 151, 50):
        print(f"-- Sampling size {size} --")
        square = square_lattice(m=size, n=size, efficiency=1, fidelity=0.975)

        # Debug
        print("Number of nodes: " + str(square.number_of_nodes()))

        diagonal = get_diagonal_pair_method(dim=2, size=8, G=square)
        thing = cumulative_running_variance(square, pair_method=diagonal, reduction_method=QNET.purify_reduce,
                                            sample_max=100)

        # Plot
        plt.plot(list(thing["sample_size"]), list(thing["e"]), label=f"e ({size}x{size})")
        plt.plot(list(thing["sample_size"]), list(thing["f"]), label=f"f ({size}x{size})")

    plt.ylabel("Standard Error")
    plt.xlabel("Number of Square Lattice Graphs Sampled (e:1, f:0.975)")
    plt.title("Error Rates of \"Simple Purify\" Against Sample Size for Different Square Lattices", fontsize=10)
    plt.legend()
    plt.show()


def std_eff_with_sample_size():
    size = 8
    e = 1
    f = 0.975
    square = square_lattice(m=size, n=size, efficiency=e, fidelity=f)

    # find maximum error for simple purify for range of sample sizes, then plot log scale
    sample_list = np.arange(100, 2001, 100)
    print(sample_list)

    for sample_size in sample_list:
        data = monte_method(square, pair_method=fixed_pair, reduction_method=QNET.purify_reduce,
                            data_method=data_method, num_iters=sample_size, num_steps=10, percolation_range=None)


std_eff_with_sample_size()
# error_expectation_against_percolation_probability()
# running_variance_test()
# running_variance_for_different_graph_sizes()
